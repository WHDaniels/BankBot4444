# -*- coding: utf-8 -*-
"""CSC4444_Group_Project_v4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19kPj3To4RH_NTHmVo03X9RkgzV20d3B-

## 1) Get data
*   Using TechWithTim's 'intents.json' as a placeholder for now.
*   In future assign group members to either (maybe both):
    *   Grab mass data through parsing or scraping
    *   Find data that has already been grabbed


## 2) Preprocessing data for use in training
*   Detailed in general when you run the cells

## 3) Creating model
*   Add input layer with the shape of [None, training[0] (length of input data)
*   Add 'fully_connected' (Dense) layer with 8 neurons
*   Add 'fully_connected' (Dense) layer with 8 neurons
*   Add output layer with 'softmax' activation
*   Add regression layer
*   Train the model

## 4) Predict based on trained model
*   Get results from trained model based on user input
*   Get predicted label from results
*   Translate predicted label to random response based on label

## 5) Add GUI
*   Can use PyQt5 for simple UI implementation

## 6) Other ideas maybe
*   Bot topic? (Brainstormed answers below, feel free to add)
  *   Casual Conversation (maybe too broad)
  *   Two bots that converse with each other (casually or about a topic)
  *   Automated shop employee
  *   List item
  *   List item
  *   List item
  *   List item
  *   List item
  *   List item
  *   List item
  *   List item
  *   List item
  *   List item
  *   List item
  *   List item
  *   List item
"""
from PyQt5.QtCore import QCoreApplication
from PyQt5.QtWidgets import QMessageBox, QFileDialog, QMainWindow, QApplication
from PyQt5 import QtCore, QtWidgets
from gui import Ui_MainWindow
import random


# gui for the program
class MainWindow(QMainWindow):
    def __init__(self, wordList, tkWord, tkLabel):
        super(MainWindow, self).__init__()
        self.ui = Ui_MainWindow()
        self.ui.setupUi(self)

        self.wordList = wordList
        self.tkWord = tkWord
        self.tkLabel = tkLabel

        self.ui.lineEdit.returnPressed.connect(self.interact)

    def appendTextToBrowser(self):
        userText = self.ui.lineEdit.text()
        self.ui.textBrowser.append("You:  " + userText)
        self.ui.lineEdit.clear()

    def interact(self):
        # while (True):

        # Get input from the user
        # print("(Press 'bye' to exit.)")
        userInput = self.ui.lineEdit.text()
        self.ui.textBrowser.append("You:  " + userInput)
        self.ui.lineEdit.clear()

        # Transform user input into a boolean list representing their
        # includence (theres a better word here that I cant think of) in the word list
        userInputArr = makeInput([userInput.replace("?", "").replace("!", "")], self.wordList, self.tkWord)

        # The results of out predictions
        results = model.predict(userInputArr)
        print(results)
        print(np.argmax(results))

        # Use the predictions to select the most likely label
        predictedLabelIndex = np.argmax(results) + 1

        for label, key in self.tkLabel.word_index.items():
            if key == predictedLabelIndex:
                for intent in data["intents"]:
                    if intent["tag"] == label:
                        response = random.choice(intent["responses"])
                        self.ui.textBrowser.append("BankBot4444:  " + response + "\n")
                        # print("\nCSC 4444 Bot: ", response)
                        # print("-" * 40, "\n")


if __name__ == "__main__":
    import sys

    # Commented out IPython magic to ensure Python compatibility.
    # %tensorflow_version 1.x

    import json
    import keras.preprocessing.text as keras
    from nltk.stem.lancaster import LancasterStemmer

    # Load the data
    with open('data/intents.json', 'r', encoding='utf-8-sig') as file:
        print(file)
        data = json.load(file)

    # Initialize our stemmer
    stemmer = LancasterStemmer()

    # Create a tokenizer for our words and our labels
    tkWord = keras.Tokenizer()
    tkLabel = keras.Tokenizer(filters='!"#$%&()*+,-./:;<=>@[\\]^`{|}~\t\n')

    # Create three lists to store our words, labels, and phrases in
    wordList = list()
    labelMatchList = list()
    phraseList = list()
    labels = list()

    # Every pattern is a phrase, so add it to the phraseList
    # Per phrase, get the label (tag) associated with that phrase
    # Get every word in each pattern and assign it to a list
    for intents in data['intents']:
        for pattern in intents['patterns']:
            phraseList.append(pattern)
            labelMatchList.append(intents['tag'])

            for word in pattern.split():
                wordList.append(stemmer.stem(word.replace('?', '')))

        # Add every label (no duplicates) to a list
        if intents['tag'] not in labels:
            labels.append(intents['tag'])

    for phrase in range(len(phraseList)):
        phraseList[phrase] = phraseList[phrase].replace('?', '')

    print("Example of wordList:")
    print(wordList, "\n")
    print("Example of labelMatchList:")
    print(labelMatchList, "\n")
    print("Example of phraseList:")
    print(phraseList, "\n")
    print("Example of the 'labels' list:")
    print(labels)

    # Fit all words in wordList
    tkWord.fit_on_texts(wordList)

    # Fit all labels in label list
    tkLabel.fit_on_texts(labelMatchList)

    # Replace each word with it's integer counterpart (derived from how often that
    # word is used compared to all the others, so most used word would be 1, etc)
    wordSeq = tkWord.texts_to_sequences(wordList)
    labelSeq = tkLabel.texts_to_sequences(labelMatchList)

    print("The 'word_index' is the frequency of each word in the list the \
    Tokenizer (tkWord/tkLabel) was fitted on.\n")
    print("Example of the word_index of tkWord:")
    print(tkWord.word_index, "\n")
    print("Example of the word_index of tkLabel:")
    print(tkLabel.word_index, "\n\n")

    print("So we can translate: ", wordList)
    print("to:                  ", wordSeq, "\n")
    print("And: ", labelMatchList)
    print("to:  ", labelSeq)


    def makeInput(phraseList, wordList, tkWord):
        """
      Converts each phrase into a list of booleans representing whether each word in
      wordList appears in the phrase.
      Returns a list of lists of boolean values.
      """
        inputList = list()

        # For each phrase
        for phrase in phraseList:

            # Create a empty list in which to store sequenced phrases
            seqPhrase = [0] * len(tkWord.word_index.keys())

            # For each word in the phrase
            for word in phrase.split():
                # Stem the word before proceeding
                word = stemmer.stem(word)

                # If the current word is in the word index of the tokenized wordList
                if word in tkWord.word_index:
                    # The index of that word in wordList is the index in seqPhrase minus 1,
                    # that will be changed from the boolean 0 to 1 (False to True)
                    seqPhrase[tkWord.word_index[word] - 1] = 1

            # Append the sequenced phrase to the input list
            inputList.append(seqPhrase)

        return inputList


    # Create a list that will become our input data to train on
    inputList = makeInput(phraseList, wordList, tkWord)


    # print("So lets translate our list of phrases into a list of booleans \
    # representing whether each word in the word list appears in the phrase.")
    # print("input:", inputList, "\n")

    def makeOutput(phraseList, tkLabel, labelMatchList):
        """
      For each phrase, creates a list that represents the label that corresponds to
      that phrase. Representation is a list of boolean values where there is one '1'
      (True) entry at the index that corresponds to the (label word index of the
      label that corresponds to the current phrase) minus 1. All lists are combined
      and returned into the entire output list.
      Returns a list of lists of boolean values.
      """
        outputList = list()

        for x in range(len(phraseList)):
            # Create a empty list in which to store sequenced phrases
            seqLabels = [0] * len(tkLabel.word_index.keys())

            # The index of the element that will be made 1 (True)
            countIndex = tkLabel.word_index[labelMatchList[x]] - 1

            # Represent the correct label's index as a boolean value
            seqLabels[countIndex] = 1

            # Add the list of booleans to the output list
            outputList.append(seqLabels)

        return outputList


    output = makeOutput(phraseList, tkLabel, labelMatchList)

    print("Now let's use this information to create a list the length of the \
    number of labels that represents the index the corresponding label is in \
    in the labels word index:")
    # print(output, "\n")

    import numpy as np

    # Transform both of these lists into numpy arrays
    inputArr = np.array(inputList)
    output = np.array(output)

    print("Let's both the input and output lists into 2D numpy arrays so they can \
    be used in training:\n")
    # print(inputArr, "\n")
    # print(output, "\n")

    print("Here are the shapes of our input and output sets:")
    print("input shape:", inputArr.shape)
    print("output shape:", output.shape)

    import tensorflow as tf
    import tflearn

    print("We can train the model by running the cell after this one.")
    print("TensorFlow will fuss at us but it's ok :)")

    print("The amount of neurons in the dense layer is the mean of the size of \
    the input and output:", (inputArr.shape[1] + output.shape[1]) / 2)

    # try:
    # Load a trained model if it exists
    # model.load("model.tflearn")

    # except:
    # tf.reset_default_graph()

    mean = int((inputArr.shape[1] + output.shape[1]) / 2)

    net = tflearn.input_data(shape=[None, len(inputArr[0])])
    net = tflearn.fully_connected(net, mean)
    net = tflearn.fully_connected(net, mean)
    net = tflearn.fully_connected(net, len(output[0]), activation="softmax")
    net = tflearn.regression(net)

    model = tflearn.DNN(net)
    model.fit(inputArr, output, n_epoch=7, show_metric=True)

    # Save the newly trained model
    model.save("model.tflearn")
    # Load a trained model
    model.load("model.tflearn")


    def interactPredict(wordList, tkWord, tkLabel):
        while (True):
            # Get input from the user
            print("\n(Press 'bye' to exit.)")
            userInput = input("You: ")

            # Exit application if 'bye' is entered
            if userInput == 'bye':
                print("\nGoodbye!\n")
                break

            # Transform user input into a boolean list representing their
            # includence (theres a better word here that I cant think of) in the word list
            userInputArr = makeInput([userInput.replace("?", "")], wordList, tkWord)

            # The results of out predictions
            results = model.predict(userInputArr)

            # Use the predictions to select the most likely label
            predictedLabelIndex = np.argmax(results) + 1
            for label, key in tkLabel.word_index.items():
                if key == predictedLabelIndex:
                    print("Predicted: ", label, "\n")
                    print("-" * 40, "\n")


    print("\nWe use the same methods we used before to convert phrases to words\n\
    for transforming the user's input into an array that can be predicted on.\n\
    We get the results of our predictions, and use that information to select the\n\
    most likely label.\n")
    print("Run the next cell to test out the label prediction accuracy.\n\
    (Keep in mind that with little data, prediction power will be limited)")

    # Test the bot for yourself
    # interactPredict(wordList, tkWord, tkLabel)

    print("Now that we have a good idea of what the bot is capable of\n\
    prediction-wise, we can implement an actual chatbot (based on the little\n\
    data we have).\n")
    print("Try it out in the cell below.")

    # interact(wordList, tkWord, tkLabel)

    # Code to make new json file (if we decide to use it)
    # --> Running will result in error until configured for colab usage <--
    """
    import json
    import pandas as pd

    X = pd.read_csv('train.csv')

    patterns = list(X['text'])
    intents = list(X['category'])
    noDupIntents = list(set(intents))

    jsonDict = {
        "intents": []
    }
    for intent in range(len(noDupIntents)):
        addDict = {
            "tag": str(noDupIntents[intent]),
            "patterns": [pattern.rstrip("\n") for n, pattern in enumerate(patterns) if intents[n] == noDupIntents[intent]],
            "responses": ["add responses here"]
        }
        jsonDict["intents"].append(addDict)

    # Serializing json
    json_object = json.dumps(jsonDict, indent=4)

    # Writing to sample.json
    with open("sample.json", "w") as outfile:
        outfile.write(json_object)
    """

    print("Let's both the input and output lists into 2D numpy arrays so they can \
        be used in training:\n")
    print(inputArr, "\n")
    print(output, "\n")

    print("Here are the shapes of our input and output sets:")
    print("input shape:", inputArr.shape)
    print("output shape:", output.shape, "\n\n")

    print(list(set(labels)))

    print("\n\nTwo dense layers with " + str(mean) +
          " (which is the average of the input and output size) neurons each.")

    import qdarkstyle

    app = QtWidgets.QApplication(sys.argv)
    w = MainWindow(wordList, tkWord, tkLabel)
    app.setStyleSheet(qdarkstyle.load_stylesheet_pyqt5())
    w.show()
    sys.exit(app.exec_())
